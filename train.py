# -*- coding: utf-8 -*-
"""image.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13amgoYSLuJ6YzHPjzoMu135Oi0TtytPR
"""

import pandas as pd
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import sklearn
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
from sklearn.metrics import mean_squared_error, r2_score
import warnings
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.applications import vgg16
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import os
import cv2
from tensorflow.keras.models import model_from_json
from tensorflow.keras.utils import img_to_array
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.base")
warnings.filterwarnings("ignore", category=FutureWarning, module="numpy.core.fromnumeric")



df = pd.read_csv('fer2013.csv')

print(df.head(5))
print("*" * 40)
print("Shape before dropping missing values:\n", df.shape)
print("\n********** Missing Values **********\n")
print(df.isnull().sum(axis=0))

# Drop missing values and assign the result back to the DataFrame
df = df.dropna()


print("\n********** Missing Values after dropping **********\n")
print(df.isnull().sum(axis=0))
print("\n********** Datatypes **********\n")
print(df.dtypes)


# Print Unique Values:
print("\n********** Unique Values **********\n")
unique_values = df['emotion'].unique()
print(f"Unique values in the 'emotion' column are:", unique_values)

# Define a dictionary mapping numeric values to emotion names
emotion_mapping = {
    0: "Angry",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Sad",
    5: "Surprise",
    6: "Neutral"
}

# Map the 'emotion' column using the defined mapping
df['emotion'] = df['emotion'].map(emotion_mapping)

# Print Unique Values after mapping:
print("\n********** Unique Values **********\n")
unique_values = df['emotion'].unique()
print(f"Unique values in the 'emotion' column are:", unique_values)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
fig, axes = plt.subplots(3, 3, figsize=(10, 10))
for i, (index, row) in enumerate(df.head(9).iterrows()):
    # Parse pixel values
    pixels = np.fromstring(row['pixels'], dtype=int, sep=' ')
    # Reshape pixel values to the original image dimensions (assuming images are 48x48)
    image_array = pixels.reshape(48, 48)

    # Display the image on the corresponding subplot
    axes[i // 3, i % 3].imshow(image_array, cmap='gray', aspect='auto')
    axes[i // 3, i % 3].set_title(f"Emotion: {row['emotion']}")

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

# Use LabelEncoder to convert string labels to numerical values
label_encoder = LabelEncoder()
df['emotion'] = label_encoder.fit_transform(df['emotion'])

# Filter rows based on 'Usage' column
train_data = df[df['Usage'] == 'Training']
val_data = df[df['Usage'] == 'PrivateTest']
test_data = df[df['Usage'] == 'PublicTest']

# Function to preprocess pixel data
def preprocess_pixels(pixel_string):
    pixels = np.fromstring(pixel_string, dtype=int, sep=' ')
    return pixels.reshape(48, 48, 1)  # Reshape to (height, width, channels=1) for grayscale

# Apply preprocessing to each dataset
train_X = np.array([preprocess_pixels(pixel_str) for pixel_str in train_data['pixels']])
val_X = np.array([preprocess_pixels(pixel_str) for pixel_str in val_data['pixels']])
test_X = np.array([preprocess_pixels(pixel_str) for pixel_str in test_data['pixels']])

# Extract emotion labels and perform one-hot encoding
train_y = to_categorical(train_data['emotion'].values, num_classes=7)
val_y = to_categorical(val_data['emotion'].values, num_classes=7)
test_y = to_categorical(test_data['emotion'].values, num_classes=7)

# Create an instance of the ImageDataGenerator with data augmentation settings
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Fit the generator on your training data
datagen.fit(train_X)

# Print the shapes of the datasets
print("Training data shape:", train_X.shape, train_y.shape)
print("Validation data shape:", val_X.shape, val_y.shape)
print("Test data shape:", test_X.shape, test_y.shape)

num_samples = train_X.shape[0]
print(f"Number of training samples: {num_samples}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Define the simplified VGG16-like model
model = Sequential()

# Convolutional Block 1
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

# Convolutional Block 2
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

# Convolutional Block 3
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Flatten())
model.add(Dense(256, activation='relu', name="block6"))
model.add(Dropout(0.5, name="drop1"))
model.add(Dense(128, activation='relu', name="block7"))
model.add(Dropout(0.3, name="drop2"))
model.add(Dense(7, activation='softmax', name="block8"))

# Compile the model
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model

checkpointer = [
    EarlyStopping(monitor='val_accuracy', verbose=1, restore_best_weights=True, mode="max", patience=10),
    ModelCheckpoint('/content/drive/MyDrive/best_model.h5', monitor="val_accuracy", verbose=1, save_best_only=True, mode="max")
]


#If runtime
best_model_path = 'best_model.h5'
best_model = load_model(best_model_path)
optimizer = Adam(learning_rate=0.0001)
best_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
history = best_model.fit(
    datagen.flow(train_X, train_y, batch_size=32),
    epochs=100,
    validation_data=(val_X, val_y),
    callbacks=checkpointer
)

'''
history = model.fit(
    datagen.flow(train_X, train_y, batch_size=32),
    epochs=100,
    validation_data=(val_X, val_y),
    callbacks=checkpointer
)
'''
# Plotting training and validation loss
plt.figure(figsize=(12, 6))

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation accuracy values

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Adjust layout for better visualization
plt.tight_layout()

# Show the plot
plt.show()

from sklearn.metrics import classification_report

# Evaluate the model on the test set
test_loss, test_accuracy = best_model.evaluate(test_X, test_y)

# Print the evaluation results
print(f'Loss: {test_loss:.4f}')
print(f'Accuracy: {test_accuracy * 100:.2f}%')

y_pred = best_model.predict(test_X)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(test_y, axis=1)

# Generate a classification report
print(classification_report(y_true_classes, y_pred_classes))

conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)

emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Plot confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=emotion_labels, yticklabels=emotion_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

from tensorflow.keras.models import load_model
best_model_path = '/content/drive/MyDrive/best_model.h5'
best_model = load_model(best_model_path)
best_model.save("best_model.h5")
best_model_json = best_model.to_json()
with open("model.json", "w") as json_file:
  json_file.write(best_model_json)

model = model_from_json(open("model.json", "r").read())
model.load_weights('best_model.h5')



